# -*- coding: utf-8 -*-
"""Vertical-FL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10pdw63KGtA5CedVtlM5pfBCfzOr1ZK72
"""

!pip install --quiet --upgrade tensorflow tensorflow-federated

import tensorflow as tf
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler

# 1. Charger les données depuis un fichier CSV
# Le fichier contient les caractéristiques des patients atteints ou non de diabète
# La colonne 'Outcome' représente la classe cible (0 : Non diabétique, 1 : Diabétique)
data = pd.read_csv('/content/drive/MyDrive/diabetes.csv', header=0)

# 2. Définir les colonnes attribuées à chaque client
# Chaque client détient une partie des caractéristiques des patients
client_columns = {
    "Client_1": ['Pregnancies', 'Age', 'BMI'],
    "Client_2": ['Glucose', 'BloodPressure', 'SkinThickness'],
    "Client_3": ['Insulin', 'DiabetesPedigreeFunction']
}

# 3. Créer les données fédérées verticalement
# Chaque client possède une partie des caractéristiques et partage les mêmes labels
def create_vertical_federated_data(data, client_columns):
    client_data = []
    labels = data['Outcome'].values.astype(np.float32).reshape(-1, 1)  # Labels communs
    for features in client_columns.values():
        client_features = data[features].copy()
        # Convertir les valeurs en numériques, gérer les NaN en remplaçant par la médiane
        client_features = client_features.apply(pd.to_numeric, errors='coerce').fillna(client_features.median())
        # Normalisation des données
        scaler = StandardScaler()
        client_features_scaled = scaler.fit_transform(client_features).astype(np.float32)
        # Création d'un dataset TensorFlow avec une taille de batch de 64
        dataset = tf.data.Dataset.from_tensor_slices((client_features_scaled, labels)).batch(64)
        client_data.append(dataset)
    return client_data

# Générer les données fédérées verticalement
vertical_federated_data = create_vertical_federated_data(data, client_columns)

# 4. Construire un modèle de réseau de neurones pour chaque client
def build_vertical_model(input_shape):
    return tf.keras.Sequential([
        tf.keras.layers.Input(shape=input_shape),
        tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)), # Régularisation L2
        tf.keras.layers.Dropout(0.5), # Désactive 50% des neurones pour éviter le surapprentissage
        tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(16, activation='relu') # Dernière couche avant fusion
    ])

# 5. Construire le modèle global en combinant les sous-modèles clients
def build_global_model(client_models, output_dim=1):
    inputs = [model.input for model in client_models]  # Liste des entrées des modèles clients
    concatenated = tf.keras.layers.concatenate([model.output for model in client_models])  # Fusion des sorties
    output = tf.keras.layers.Dense(output_dim, activation='sigmoid')(concatenated)  # Couche finale de classification
    return tf.keras.Model(inputs=inputs, outputs=output)

# Initialiser les modèles clients avec des tailles d'entrée correspondant aux colonnes respectives
client_models = [build_vertical_model(input_shape=(len(features),)) for features in client_columns.values()]

# Construire le modèle global
global_model = build_global_model(client_models)

# Compiler le modèle avec l'optimiseur Adam et la fonction de perte binaire
# Fonction de perte 'binary_crossentropy' car c'est une classification binaire
global_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                     loss='binary_crossentropy',
                     metrics=['accuracy'])

# 6. Entraînement du modèle fédéré
epochs = 100
for epoch in range(epochs):
    print(f"Epoch {epoch + 1}/{epochs}")
    epoch_loss = 0
    epoch_accuracy = 0
    batch_count = 0

    for batch in zip(*vertical_federated_data):  # Combine les batches de tous les clients
        inputs = [np.array(x) for x, _ in batch]  # Extraire les caractéristiques de chaque client
        labels = np.array(batch[0][1])  # Les labels sont les mêmes pour tous les clients

        # Mise à jour du modèle global avec chaque mini-batch
        batch_loss, batch_accuracy = global_model.train_on_batch(inputs, labels)
        epoch_loss += batch_loss
        epoch_accuracy += batch_accuracy
        batch_count += 1

    # Affichage des métriques moyennes pour l'époque
    epoch_loss /= batch_count
    epoch_accuracy /= batch_count
    print(f"Epoch {epoch + 1}: Loss = {epoch_loss:.4f}, Accuracy = {epoch_accuracy:.4f}")

    # Arrêt de l'entraînement si la précision atteint 99%
    if epoch_accuracy >= 0.99:
        print("Précision atteinte à 99%, arrêt de l'entraînement.")
        break

# 7. Évaluation finale du modèle global
# Combiner toutes les données des clients pour l'évaluation
data_inputs = [np.vstack([x.numpy() for x, _ in data]) for data in vertical_federated_data]
labels = np.vstack([y.numpy() for _, y in vertical_federated_data[0]])  # Labels communs
loss, accuracy = global_model.evaluate(data_inputs, labels, verbose=0)
print(f"Final Global Model - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}")